{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python gpt4all firecrawl-py transformers torch einops atlassian-python-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from getpass import getpass\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "LANGCHAIN_ENDPOINT = os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "LANGCHAIN_API_KEY = os.environ['LANGCHAIN_API_KEY']\n",
    "FIRECRAWL_API_KEY = os.environ['FIRECRAWL_API_KEY']\n",
    "NOTION_API_KEY = os.environ['NOTION_API_KEY']\n",
    "NOTION_DB_ID = os.environ['NOTION_DB_ID']\n",
    "JIRA_API_KEY = os.environ['JIRA_API_KEY']\n",
    "ATLASSIAN_USERNAME = os.environ[\"JIRA_USERNAME\"]\n",
    "ATLASSIAN_URL = os.environ[\"JIRA_INSTANCE_URL\"]\n",
    "JIRA_CLOUD_ID = os.environ[\"JIRA_CLOUD_ID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "from langchain_community.agent_toolkits.jira.toolkit import JiraToolkit\n",
    "from langchain_community.utilities.jira import JiraAPIWrapper\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:latest\", format=\"json\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josuesanchez/Personal Project/chatbot/myenv/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The function `initialize_agent` was deprecated in LangChain 0.1.0 and will be removed in 0.3.0. Use Use new agent constructor methods like create_react_agent, create_json_agent, create_structured_chat_agent, etc. instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Jira API wrapper\n",
    "jira = JiraAPIWrapper(\n",
    "    jira_api_token=JIRA_API_KEY,\n",
    "    jira_instance_url=ATLASSIAN_URL,\n",
    "    jira_cloud=JIRA_CLOUD_ID\n",
    ")\n",
    "\n",
    "# Create the toolkit from the Jira API wrapper\n",
    "toolkit = JiraToolkit.from_jira_api_wrapper(jira)\n",
    "\n",
    "# Initialize the agent with the toolkit and LLM, enabling parsing error handling\n",
    "agent = initialize_agent(\n",
    "    toolkit.get_tools(),\n",
    "    llm,\n",
    "    agent=AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True  # Enable error handling to retry on parsing errors\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m{\n",
      "  \"action\": \"Create Issue\",\n",
      "  \"action_input\": {\n",
      "    \"summary\": \"Make More Fried Rice\",\n",
      "    \"description\": \"Reminder to make more delicious fried rice for dinner\",\n",
      "    \"issuetype\": {\"name\": \"Task\"},\n",
      "    \"priority\": {\"name\": \"Low\"},\n",
      "    \"project\": {\"key\": \"PW\"}\n",
      "  }\n",
      "}\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'make a new issue in project PW to remind me to make more fried rice',\n",
       " 'output': '{\\n  \"action\": \"Create Issue\",\\n  \"action_input\": {\\n    \"summary\": \"Make More Fried Rice\",\\n    \"description\": \"Reminder to make more delicious fried rice for dinner\",\\n    \"issuetype\": {\"name\": \"Task\"},\\n    \"priority\": {\"name\": \"Low\"},\\n    \"project\": {\"key\": \"PW\"}\\n  }\\n}'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke(\"make a new issue in project PW to remind me to make more fried rice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import NotionDBLoader\n",
    "\n",
    "# Load documents with firecrawl a web scraper tool\n",
    "urls = [\n",
    "    \"https://www.tokyotechies.com\",\n",
    "    \"https://www.tokyotechies.com/about-us\",\n",
    "    \"https://www.tokyotechies.com/solutions/kotae\"\n",
    "]\n",
    "docs = [FireCrawlLoader(api_key=FIRECRAWL_API_KEY, url=url, mode=\"scrape\").load() for url in urls]\n",
    "\n",
    "\n",
    "# Flatten the list of documents\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Filter out complex metadata\n",
    "filtered_docs = []\n",
    "for doc in doc_splits:\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
    "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n",
    "\n",
    "# Save the filtered_docs to a file or cache for later use\n",
    "import pickle\n",
    "\n",
    "with open('filtered_docs.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('filtered_docs.pkl', 'rb') as f:\n",
    "    filtered_docs = pickle.load(f)\n",
    "\n",
    "print(filtered_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josuesanchez/Personal Project/chatbot/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
      "- configuration_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
      "- modeling_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/Users/josuesanchez/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/e55a7d4324f65581af5f483e830b80f34680e8ff/modeling_hf_nomic_bert.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loader(resolved_archive_file)\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Load the filtered_docs from the saved file\n",
    "import pickle\n",
    "\n",
    "with open('filtered_docs.pkl', 'rb') as f:\n",
    "    filtered_docs = pickle.load(f)\n",
    "\n",
    "# Load the embedding model and tokenizer\n",
    "model_name = \"nomic-ai/nomic-embed-text-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Function to generate embeddings using the loaded model\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].cpu().numpy().tolist()  # Convert ndarray to list\n",
    "\n",
    "# Wrapper class to use with Chroma\n",
    "class CustomEmbedding:\n",
    "    def embed_documents(self, texts):\n",
    "        return [embed_text(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return embed_text(text)\n",
    "\n",
    "# Instantiate the embedding class\n",
    "custom_embedding = CustomEmbedding()\n",
    "\n",
    "# Add documents with embeddings to the vectorDB using the embedding class\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=custom_embedding,  # Use the embedding class instance\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assesing \n",
    "    relevance of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n \n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the docuemnt is relevant to the question. \\n \n",
    "    Providde the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document* \\n\\n {document} \\n\\n\n",
    "    Here is the user question* {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"document\"]\n",
    ")\n",
    "\n",
    "# Chain the prompt, LLM, and output parser together\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Define the user question\n",
    "question = \"What is Tokyo Techies?\"\n",
    "\n",
    "# Retrieve documents related to the question\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "# Get the content of the second retrieved document\n",
    "doc_txt = docs[1].page_content  # Use page_content instead of page_context\n",
    "\n",
    "# Grade the relevance of the document\n",
    "result = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"Kotae is a chatbot platform that allows small businesses to automate conversations and delight customers. It can be trained using a company's knowledge base, website scrapes, training files, and FAQs.\" \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assitant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\",\n",
    "input_variables=[\"question\", \"document\"]\n",
    ")\n",
    "\n",
    "#Post processing\n",
    "def format_doc(docs):\n",
    "    return\"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "#Run\n",
    "\n",
    "question = \"What do you know about kotae?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assesing whether\n",
    "    an answer is grounded in / supported by a set of facts. Give binary scores 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preambel or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n -------- \\n\n",
    "    {documents}\n",
    "    \\n -------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\",\n",
    "input_variables=[\"generation\", \"document\"]\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer grader\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assesing whether\n",
    "    the answer is useful in resolve a question. Give binary scores 'yes' or 'no' score to indicate \n",
    "    whether the answer is use to resolve a question. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preambel or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the answer:\n",
    "    \\n -------- \\n\n",
    "    {generation}\n",
    "    \\n -------- \\n\n",
    "    Here is the question: {question}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\",\n",
    "input_variables=[\"generation\", \"question\"]\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce269a90fb56c0c7841b9bbe80dbf445b85e73251be9a478855c63ed1bb92888"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
