{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-nomic\n",
      "  Using cached langchain_nomic-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting langchain_community\n",
      "  Using cached langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting langchainhub\n",
      "  Using cached langchainhub-0.1.20-py3-none-any.whl.metadata (659 bytes)\n",
      "Collecting chromadb\n",
      "  Using cached chromadb-0.5.5-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: langchain in ./myenv/lib/python3.12/site-packages (0.2.12)\n",
      "Collecting langgraph\n",
      "  Using cached langgraph-0.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting tavily-python\n",
      "  Using cached tavily_python-0.3.7-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gpt4all\n",
      "  Using cached gpt4all-2.8.0-py3-none-macosx_10_15_universal2.whl.metadata (4.7 kB)\n",
      "Collecting firecrawl-py\n",
      "  Using cached firecrawl_py-0.0.20-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.0-cp312-none-macosx_11_0_arm64.whl.metadata (26 kB)\n",
      "Collecting einops\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: langchain-core<0.3,>=0.1.46 in ./myenv/lib/python3.12/site-packages (from langchain-nomic) (0.2.29)\n",
      "Collecting nomic<4.0.0,>=3.0.29 (from langchain-nomic)\n",
      "  Using cached nomic-3.1.1.tar.gz (45 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pillow<11.0.0,>=10.3.0 (from langchain-nomic)\n",
      "  Downloading pillow-10.4.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./myenv/lib/python3.12/site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./myenv/lib/python3.12/site-packages (from langchain_community) (2.0.32)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./myenv/lib/python3.12/site-packages (from langchain_community) (3.10.2)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Using cached dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in ./myenv/lib/python3.12/site-packages (from langchain_community) (0.1.98)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in ./myenv/lib/python3.12/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: requests<3,>=2 in ./myenv/lib/python3.12/site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in ./myenv/lib/python3.12/site-packages (from langchain_community) (8.5.0)\n",
      "Collecting regex>=2022.1.18 (from tiktoken)\n",
      "  Downloading regex-2024.7.24-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging<25,>=23.2 in ./myenv/lib/python3.12/site-packages (from langchainhub) (24.1)\n",
      "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
      "  Using cached types_requests-2.32.0.20240712-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting build>=1.0.3 (from chromadb)\n",
      "  Using cached build-1.2.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: pydantic>=1.9 in ./myenv/lib/python3.12/site-packages (from chromadb) (2.8.2)\n",
      "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
      "  Downloading chroma_hnswlib-0.7.6-cp312-cp312-macosx_11_0_arm64.whl.metadata (252 bytes)\n",
      "Collecting fastapi>=0.95.2 (from chromadb)\n",
      "  Using cached fastapi-0.112.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached uvicorn-0.30.5-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting posthog>=2.4.0 (from chromadb)\n",
      "  Using cached posthog-3.5.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./myenv/lib/python3.12/site-packages (from chromadb) (4.12.2)\n",
      "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
      "  Downloading onnxruntime-1.18.1-cp312-cp312-macosx_11_0_universal2.whl.metadata (4.3 kB)\n",
      "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
      "  Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.20.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting pypika>=0.48.9 (from chromadb)\n",
      "  Using cached PyPika-0.48.9.tar.gz (67 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting tqdm>=4.65.0 (from chromadb)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting overrides>=7.3.1 (from chromadb)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting importlib-resources (from chromadb)\n",
      "  Using cached importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting grpcio>=1.58.0 (from chromadb)\n",
      "  Downloading grpcio-1.65.4-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting bcrypt>=4.0.1 (from chromadb)\n",
      "  Using cached bcrypt-4.2.0-cp39-abi3-macosx_10_12_universal2.whl.metadata (9.6 kB)\n",
      "Collecting typer>=0.9.0 (from chromadb)\n",
      "  Using cached typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting kubernetes>=28.1.0 (from chromadb)\n",
      "  Using cached kubernetes-30.1.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting mmh3>=4.0.1 (from chromadb)\n",
      "  Downloading mmh3-4.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: orjson>=3.9.12 in ./myenv/lib/python3.12/site-packages (from chromadb) (3.10.7)\n",
      "Collecting httpx>=0.27.0 (from chromadb)\n",
      "  Using cached httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in ./myenv/lib/python3.12/site-packages (from langchain) (0.2.2)\n",
      "Collecting langgraph-checkpoint<2.0.0,>=1.0.2 (from langgraph)\n",
      "  Using cached langgraph_checkpoint-1.0.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers>=0.13.2 (from chromadb)\n",
      "  Downloading tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-72.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./myenv/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
      "  Using cached pyproject_hooks-1.1.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting starlette<0.38.0,>=0.37.2 (from fastapi>=0.95.2->chromadb)\n",
      "  Using cached starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting anyio (from httpx>=0.27.0->chromadb)\n",
      "  Using cached anyio-4.4.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: certifi in ./myenv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (2024.7.4)\n",
      "Collecting httpcore==1.* (from httpx>=0.27.0->chromadb)\n",
      "  Using cached httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: idna in ./myenv/lib/python3.12/site-packages (from httpx>=0.27.0->chromadb) (3.7)\n",
      "Collecting sniffio (from httpx>=0.27.0->chromadb)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx>=0.27.0->chromadb)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in ./myenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in ./myenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Collecting google-auth>=1.0.1 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading google_auth-2.33.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 (from kubernetes>=28.1.0->chromadb)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting requests-oauthlib (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting oauthlib>=3.2.2 (from kubernetes>=28.1.0->chromadb)\n",
      "  Downloading oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in ./myenv/lib/python3.12/site-packages (from kubernetes>=28.1.0->chromadb) (2.2.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./myenv/lib/python3.12/site-packages (from langchain-core<0.3,>=0.1.46->langchain-nomic) (1.33)\n",
      "Collecting click (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting jsonlines (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Using cached jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting loguru (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Using cached loguru-0.7.2-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting rich (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting pandas (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting pyarrow (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting pyjwt (from nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Using cached PyJWT-2.9.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-5.27.3-cp38-abi3-macosx_10_9_universal2.whl.metadata (592 bytes)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting importlib-metadata<=8.0.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached importlib_metadata-8.0.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.26.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
      "  Using cached opentelemetry_proto-1.26.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
      "  Downloading protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl.metadata (541 bytes)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Downloading wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.47b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
      "  Using cached asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./myenv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in ./myenv/lib/python3.12/site-packages (from pydantic>=1.9->chromadb) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./myenv/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Collecting shellingham>=1.3.0 (from typer>=0.9.0->chromadb)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting httptools>=0.5.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading httptools-0.6.1-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.6 kB)\n",
      "Collecting python-dotenv>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading uvloop-0.19.0-cp312-cp312-macosx_10_9_universal2.whl.metadata (4.9 kB)\n",
      "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading watchfiles-0.23.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting websockets>=10.4 (from uvicorn[standard]>=0.18.3->chromadb)\n",
      "  Downloading websockets-12.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl.metadata (3.0 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading cachetools-5.4.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata<=8.0.0,>=6.0->opentelemetry-api>=1.2.0->chromadb)\n",
      "  Using cached zipp-3.19.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./myenv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.46->langchain-nomic) (3.0.0)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./myenv/lib/python3.12/site-packages (from rich->nomic<4.0.0,>=3.0.29->langchain-nomic) (2.18.0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting pytz>=2020.1 (from pandas->nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading pytz-2024.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->nomic<4.0.0,>=3.0.29->langchain-nomic)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Using cached langchain_nomic-0.1.2-py3-none-any.whl (3.8 kB)\n",
      "Using cached langchain_community-0.2.11-py3-none-any.whl (2.3 MB)\n",
      "Downloading tiktoken-0.7.0-cp312-cp312-macosx_11_0_arm64.whl (906 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.7/906.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached langchainhub-0.1.20-py3-none-any.whl (5.0 kB)\n",
      "Using cached chromadb-0.5.5-py3-none-any.whl (584 kB)\n",
      "Downloading chroma_hnswlib-0.7.6-cp312-cp312-macosx_11_0_arm64.whl (185 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.4/185.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached langgraph-0.2.3-py3-none-any.whl (81 kB)\n",
      "Using cached tavily_python-0.3.7-py3-none-any.whl (13 kB)\n",
      "Using cached gpt4all-2.8.0-py3-none-macosx_10_15_universal2.whl (6.5 MB)\n",
      "Using cached firecrawl_py-0.0.20-py3-none-any.whl (9.5 kB)\n",
      "Using cached transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
      "Downloading torch-2.4.0-cp312-none-macosx_11_0_arm64.whl (62.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Using cached bcrypt-4.2.0-cp39-abi3-macosx_10_12_universal2.whl (472 kB)\n",
      "Using cached build-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Using cached fastapi-0.112.0-py3-none-any.whl (93 kB)\n",
      "Downloading grpcio-1.65.4-cp312-cp312-macosx_10_9_universal2.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "Using cached httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
      "Using cached huggingface_hub-0.24.5-py3-none-any.whl (417 kB)\n",
      "Using cached fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Using cached kubernetes-30.1.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Using cached langgraph_checkpoint-1.0.2-py3-none-any.whl (14 kB)\n",
      "Downloading mmh3-4.1.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading onnxruntime-1.18.1-cp312-cp312-macosx_11_0_universal2.whl (15.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached opentelemetry_api-1.26.0-py3-none-any.whl (61 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_grpc-1.26.0-py3-none-any.whl (18 kB)\n",
      "Using cached opentelemetry_exporter_otlp_proto_common-1.26.0-py3-none-any.whl (17 kB)\n",
      "Using cached opentelemetry_proto-1.26.0-py3-none-any.whl (52 kB)\n",
      "Using cached opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl (11 kB)\n",
      "Using cached opentelemetry_instrumentation-0.47b0-py3-none-any.whl (29 kB)\n",
      "Using cached opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl (15 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl (138 kB)\n",
      "Using cached opentelemetry_util_http-0.47b0-py3-none-any.whl (6.9 kB)\n",
      "Using cached opentelemetry_sdk-1.26.0-py3-none-any.whl (109 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Downloading pillow-10.4.0-cp312-cp312-macosx_11_0_arm64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
      "Downloading regex-2024.7.24-cp312-cp312-macosx_11_0_arm64.whl (279 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m279.1/279.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.4-cp312-cp312-macosx_11_0_arm64.whl (381 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m381.8/381.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-72.1.0-py3-none-any.whl (2.3 MB)\n",
      "Downloading tokenizers-0.19.1-cp312-cp312-macosx_11_0_arm64.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "Using cached types_requests-2.32.0.20240712-py3-none-any.whl (15 kB)\n",
      "Using cached uvicorn-0.30.5-py3-none-any.whl (62 kB)\n",
      "Using cached filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Using cached importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading google_auth-2.33.0-py2.py3-none-any.whl (200 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Downloading httptools-0.6.1-cp312-cp312-macosx_10_9_universal2.whl (146 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.4/146.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached importlib_metadata-8.0.0-py3-none-any.whl (24 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp312-cp312-macosx_10_9_universal2.whl (18 kB)\n",
      "Using cached marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
      "Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Downloading protobuf-4.25.4-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.2/394.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Using cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached starlette-0.37.2-py3-none-any.whl (71 kB)\n",
      "Using cached anyio-4.4.0-py3-none-any.whl (86 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading uvloop-0.19.0-cp312-cp312-macosx_10_9_universal2.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading watchfiles-0.23.0-cp312-cp312-macosx_11_0_arm64.whl (368 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.9/368.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading websockets-12.0-cp312-cp312-macosx_11_0_arm64.whl (121 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Downloading flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Using cached jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Using cached loguru-0.7.2-py3-none-any.whl (62 kB)\n",
      "Downloading pandas-2.2.2-cp312-cp312-macosx_11_0_arm64.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp312-cp312-macosx_11_0_arm64.whl (27.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.2/27.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached PyJWT-2.9.0-py3-none-any.whl (22 kB)\n",
      "Using cached pyproject_hooks-1.1.0-py3-none-any.whl (9.2 kB)\n",
      "Downloading requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
      "Downloading cachetools-5.4.0-py3-none-any.whl (9.5 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.2/181.2 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.5/505.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading wrapt-1.16.0-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached zipp-3.19.2-py3-none-any.whl (9.0 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.3/85.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: nomic, pypika\n",
      "  Building wheel for nomic (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nomic: filename=nomic-3.1.1-py3-none-any.whl size=46187 sha256=8ed7922006b920fd1e22c6f11ecca310917d786d2a2128cfc63340b2a4e1a975\n",
      "  Stored in directory: /Users/josuesanchez/Library/Caches/pip/wheels/96/38/d4/97917251d436771fd9e9cbab35e0ceb46edcace8f99a115995\n",
      "  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=23ec4acd04979fbc713706698c7a98d6bb265401909f1169b61ff451cd8654b3\n",
      "  Stored in directory: /Users/josuesanchez/Library/Caches/pip/wheels/d5/3d/69/8d68d249cd3de2584f226e27fd431d6344f7d70fd856ebd01b\n",
      "Successfully built nomic pypika\n",
      "Installing collected packages: pytz, pypika, mpmath, monotonic, mmh3, flatbuffers, zipp, wrapt, websockets, websocket-client, uvloop, tzdata, types-requests, tqdm, sympy, sniffio, shellingham, setuptools, safetensors, regex, python-dotenv, pyproject_hooks, pyjwt, pyasn1, pyarrow, protobuf, pillow, overrides, opentelemetry-util-http, oauthlib, networkx, mypy-extensions, mdurl, marshmallow, MarkupSafe, loguru, jsonlines, importlib-resources, humanfriendly, httptools, h11, grpcio, fsspec, filelock, einops, click, chroma-hnswlib, cachetools, bcrypt, backoff, asgiref, uvicorn, typing-inspect, tiktoken, rsa, requests-oauthlib, pyasn1-modules, posthog, pandas, opentelemetry-proto, markdown-it-py, langchainhub, jinja2, importlib-metadata, huggingface-hub, httpcore, gpt4all, googleapis-common-protos, firecrawl-py, deprecated, coloredlogs, build, anyio, watchfiles, torch, tokenizers, starlette, rich, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, httpx, google-auth, dataclasses-json, typer, transformers, tavily-python, opentelemetry-semantic-conventions, opentelemetry-instrumentation, nomic, kubernetes, fastapi, opentelemetry-sdk, opentelemetry-instrumentation-asgi, langgraph-checkpoint, langchain-nomic, opentelemetry-instrumentation-fastapi, opentelemetry-exporter-otlp-proto-grpc, langgraph, langchain_community, chromadb\n",
      "Successfully installed MarkupSafe-2.1.5 anyio-4.4.0 asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.0 build-1.2.1 cachetools-5.4.0 chroma-hnswlib-0.7.6 chromadb-0.5.5 click-8.1.7 coloredlogs-15.0.1 dataclasses-json-0.6.7 deprecated-1.2.14 einops-0.8.0 fastapi-0.112.0 filelock-3.15.4 firecrawl-py-0.0.20 flatbuffers-24.3.25 fsspec-2024.6.1 google-auth-2.33.0 googleapis-common-protos-1.63.2 gpt4all-2.8.0 grpcio-1.65.4 h11-0.14.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.24.5 humanfriendly-10.0 importlib-metadata-8.0.0 importlib-resources-6.4.0 jinja2-3.1.4 jsonlines-4.0.0 kubernetes-30.1.0 langchain-nomic-0.1.2 langchain_community-0.2.11 langchainhub-0.1.20 langgraph-0.2.3 langgraph-checkpoint-1.0.2 loguru-0.7.2 markdown-it-py-3.0.0 marshmallow-3.21.3 mdurl-0.1.2 mmh3-4.1.0 monotonic-1.6 mpmath-1.3.0 mypy-extensions-1.0.0 networkx-3.3 nomic-3.1.1 oauthlib-3.2.2 onnxruntime-1.18.1 opentelemetry-api-1.26.0 opentelemetry-exporter-otlp-proto-common-1.26.0 opentelemetry-exporter-otlp-proto-grpc-1.26.0 opentelemetry-instrumentation-0.47b0 opentelemetry-instrumentation-asgi-0.47b0 opentelemetry-instrumentation-fastapi-0.47b0 opentelemetry-proto-1.26.0 opentelemetry-sdk-1.26.0 opentelemetry-semantic-conventions-0.47b0 opentelemetry-util-http-0.47b0 overrides-7.7.0 pandas-2.2.2 pillow-10.4.0 posthog-3.5.0 protobuf-4.25.4 pyarrow-17.0.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 pyjwt-2.9.0 pypika-0.48.9 pyproject_hooks-1.1.0 python-dotenv-1.0.1 pytz-2024.1 regex-2024.7.24 requests-oauthlib-2.0.0 rich-13.7.1 rsa-4.9 safetensors-0.4.4 setuptools-72.1.0 shellingham-1.5.4 sniffio-1.3.1 starlette-0.37.2 sympy-1.13.1 tavily-python-0.3.7 tiktoken-0.7.0 tokenizers-0.19.1 torch-2.4.0 tqdm-4.66.5 transformers-4.44.0 typer-0.12.3 types-requests-2.32.0.20240712 typing-inspect-0.9.0 tzdata-2024.1 uvicorn-0.30.5 uvloop-0.19.0 watchfiles-0.23.0 websocket-client-1.8.0 websockets-12.0 wrapt-1.16.0 zipp-3.19.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -U langchain-nomic langchain_community tiktoken langchainhub chromadb langchain langgraph tavily-python gpt4all firecrawl-py transformers torch einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "LANGCHAIN_ENDPOINT = os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "LANGCHAIN_API_KEY = os.environ['LANGCHAIN_API_KEY']\n",
    "FIRECRAWL_API_KEY = os.environ['FIRECRAWL_API_KEY']\n",
    "NOTION_API_KEY = os.environ['NOTION_API_KEY']\n",
    "NOTION_DB_ID = os.environ['NOTION_DB_ID']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "llm = ChatOllama(model=\"llama3.1:latest\", format=\"json\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://api.notion.com/v1/databases/https://www.notion.so/Cron-s-help-center-cbff248a6a9040c1a90bf5d6bbac9aba?pvs=4/query",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 20\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Load documents with firecrawl a web scraper tool\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# urls = [\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m#     \"https://www.tokyotechies.com\",\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# ]\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m# docs = [FireCrawlLoader(api_key=FIRECRAWL_API_KEY, url=url, mode=\"scrape\").load() for url in urls]\u001b[39;00m\n\u001b[1;32m     14\u001b[0m loader \u001b[39m=\u001b[39m NotionDBLoader(\n\u001b[1;32m     15\u001b[0m     integration_token\u001b[39m=\u001b[39mNOTION_API_KEY,\n\u001b[1;32m     16\u001b[0m     database_id\u001b[39m=\u001b[39mNOTION_DB_ID,\n\u001b[1;32m     17\u001b[0m     request_timeout_sec\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m,  \u001b[39m# optional, defaults to 10\u001b[39;00m\n\u001b[1;32m     18\u001b[0m )\n\u001b[0;32m---> 20\u001b[0m docs \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m     22\u001b[0m \u001b[39m# Flatten the list of documents\u001b[39;00m\n\u001b[1;32m     23\u001b[0m docs_list \u001b[39m=\u001b[39m [item \u001b[39mfor\u001b[39;00m sublist \u001b[39min\u001b[39;00m docs \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m sublist]\n",
      "File \u001b[0;32m~/Personal Project/chatbot/myenv/lib/python3.12/site-packages/langchain_community/document_loaders/notiondb.py:70\u001b[0m, in \u001b[0;36mNotionDBLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Document]:\n\u001b[1;32m     66\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load documents from the Notion database.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m        List[Document]: List of documents.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     page_summaries \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_retrieve_page_summaries()\n\u001b[1;32m     71\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload_page(page_summary) \u001b[39mfor\u001b[39;00m page_summary \u001b[39min\u001b[39;00m page_summaries)\n",
      "File \u001b[0;32m~/Personal Project/chatbot/myenv/lib/python3.12/site-packages/langchain_community/document_loaders/notiondb.py:83\u001b[0m, in \u001b[0;36mNotionDBLoader._retrieve_page_summaries\u001b[0;34m(self, query_dict)\u001b[0m\n\u001b[1;32m     80\u001b[0m pages: List[Dict[\u001b[39mstr\u001b[39m, Any]] \u001b[39m=\u001b[39m []\n\u001b[1;32m     82\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m     84\u001b[0m         DATABASE_URL\u001b[39m.\u001b[39;49mformat(database_id\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdatabase_id),\n\u001b[1;32m     85\u001b[0m         method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mPOST\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     86\u001b[0m         query_dict\u001b[39m=\u001b[39;49mquery_dict,\n\u001b[1;32m     87\u001b[0m         filter_object\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilter_object,\n\u001b[1;32m     88\u001b[0m     )\n\u001b[1;32m     90\u001b[0m     pages\u001b[39m.\u001b[39mextend(data\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mresults\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m data\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mhas_more\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Personal Project/chatbot/myenv/lib/python3.12/site-packages/langchain_community/document_loaders/notiondb.py:229\u001b[0m, in \u001b[0;36mNotionDBLoader._request\u001b[0;34m(self, url, method, query_dict, filter_object)\u001b[0m\n\u001b[1;32m    221\u001b[0m     json_payload[\u001b[39m\"\u001b[39m\u001b[39mfilter\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m filter_object\n\u001b[1;32m    222\u001b[0m res \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mrequest(\n\u001b[1;32m    223\u001b[0m     method,\n\u001b[1;32m    224\u001b[0m     url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_timeout_sec,\n\u001b[1;32m    228\u001b[0m )\n\u001b[0;32m--> 229\u001b[0m res\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    230\u001b[0m \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39mjson()\n",
      "File \u001b[0;32m~/Personal Project/chatbot/myenv/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     http_error_msg \u001b[39m=\u001b[39m (\n\u001b[1;32m   1020\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Server Error: \u001b[39m\u001b[39m{\u001b[39;00mreason\u001b[39m}\u001b[39;00m\u001b[39m for url: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1023\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://api.notion.com/v1/databases/https://www.notion.so/Cron-s-help-center-cbff248a6a9040c1a90bf5d6bbac9aba?pvs=4/query"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import FireCrawlLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import NotionDBLoader\n",
    "\n",
    "# Load documents with firecrawl a web scraper tool\n",
    "# urls = [\n",
    "#     \"https://www.tokyotechies.com\",\n",
    "#     \"https://www.tokyotechies.com/about-us\",\n",
    "#     \"https://www.tokyotechies.com/solutions/kotae\"\n",
    "# ]\n",
    "# docs = [FireCrawlLoader(api_key=FIRECRAWL_API_KEY, url=url, mode=\"scrape\").load() for url in urls]\n",
    "\n",
    "loader = NotionDBLoader(\n",
    "    integration_token=NOTION_API_KEY,\n",
    "    database_id=NOTION_DB_ID,\n",
    "    request_timeout_sec=30,  # optional, defaults to 10\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "# Flatten the list of documents\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "# Split documents\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Filter out complex metadata\n",
    "filtered_docs = []\n",
    "for doc in doc_splits:\n",
    "    if isinstance(doc, Document) and hasattr(doc, 'metadata'):\n",
    "        clean_metadata = {k: v for k, v in doc.metadata.items() if isinstance(v, (str, int, float, bool))}\n",
    "        filtered_docs.append(Document(page_content=doc.page_content, metadata=clean_metadata))\n",
    "\n",
    "# Save the filtered_docs to a file or cache for later use\n",
    "import pickle\n",
    "\n",
    "with open('filtered_docs.pkl', 'wb') as f:\n",
    "    pickle.dump(filtered_docs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('filtered_docs.pkl', 'rb') as f:\n",
    "    filtered_docs = pickle.load(f)\n",
    "\n",
    "print(filtered_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josuesanchez/Personal Project/chatbot/myenv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
      "- configuration_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "A new version of the following files was downloaded from https://huggingface.co/nomic-ai/nomic-bert-2048:\n",
      "- modeling_hf_nomic_bert.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "/Users/josuesanchez/.cache/huggingface/modules/transformers_modules/nomic-ai/nomic-bert-2048/e55a7d4324f65581af5f483e830b80f34680e8ff/modeling_hf_nomic_bert.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = loader(resolved_archive_file)\n",
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Load the filtered_docs from the saved file\n",
    "import pickle\n",
    "\n",
    "with open('filtered_docs.pkl', 'rb') as f:\n",
    "    filtered_docs = pickle.load(f)\n",
    "\n",
    "# Load the embedding model and tokenizer\n",
    "model_name = \"nomic-ai/nomic-embed-text-v1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Function to generate embeddings using the loaded model\n",
    "def embed_text(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        embeddings = model(**inputs).last_hidden_state.mean(dim=1)\n",
    "    return embeddings[0].cpu().numpy().tolist()  # Convert ndarray to list\n",
    "\n",
    "# Wrapper class to use with Chroma\n",
    "class CustomEmbedding:\n",
    "    def embed_documents(self, texts):\n",
    "        return [embed_text(text) for text in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return embed_text(text)\n",
    "\n",
    "# Instantiate the embedding class\n",
    "custom_embedding = CustomEmbedding()\n",
    "\n",
    "# Add documents with embeddings to the vectorDB using the embedding class\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=filtered_docs,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=custom_embedding,  # Use the embedding class instance\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'score': 'yes'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assesing \n",
    "    relevance of a retrieved document to a user question. If the document contains keywords related to the user question,\n",
    "    grade it as relevant. It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \\n \n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the docuemnt is relevant to the question. \\n \n",
    "    Providde the binary score as a JSON with a single key 'score' and no premable or explaination.\n",
    "    <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the retrieved document* \\n\\n {document} \\n\\n\n",
    "    Here is the user question* {question} \\n <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\",\n",
    "    input_variables=[\"question\", \"document\"]\n",
    ")\n",
    "\n",
    "# Chain the prompt, LLM, and output parser together\n",
    "retrieval_grader = prompt | llm | JsonOutputParser()\n",
    "\n",
    "# Define the user question\n",
    "question = \"What is Tokyo Techies?\"\n",
    "\n",
    "# Retrieve documents related to the question\n",
    "docs = retriever.invoke(question)\n",
    "\n",
    "# Get the content of the second retrieved document\n",
    "doc_txt = docs[1].page_content  # Use page_content instead of page_context\n",
    "\n",
    "# Grade the relevance of the document\n",
    "result = retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"Kotae is a chatbot platform that allows small businesses to automate conversations and delight customers. It can be trained using a company's knowledge base, website scrapes, training files, and FAQs.\" \n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are an assitant for question-answering tasks.\n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know.\n",
    "    Use three sentences maximum and keep the answer concise <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Question: {question}\n",
    "    Context: {context}\n",
    "    Answer: <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\",\n",
    "input_variables=[\"question\", \"document\"]\n",
    ")\n",
    "\n",
    "#Post processing\n",
    "def format_doc(docs):\n",
    "    return\"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "#Run\n",
    "\n",
    "question = \"What do you know about kotae?\"\n",
    "docs = retriever.invoke(question)\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Hallucination Grader\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assesing whether\n",
    "    an answer is grounded in / supported by a set of facts. Give binary scores 'yes' or 'no' score to indicate \n",
    "    whether the answer is grounded in / supported by a set of facts. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preambel or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here are the facts:\n",
    "    \\n -------- \\n\n",
    "    {documents}\n",
    "    \\n -------- \\n\n",
    "    Here is the answer: {generation}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\",\n",
    "input_variables=[\"generation\", \"document\"]\n",
    ")\n",
    "\n",
    "hallucination_grader = prompt | llm | JsonOutputParser()\n",
    "hallucination_grader.invoke({\"documents\": docs, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 'yes'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Answer grader\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|> You are a grader assesing whether\n",
    "    the answer is useful in resolve a question. Give binary scores 'yes' or 'no' score to indicate \n",
    "    whether the answer is use to resolve a question. Provide the binary score as a JSON with a \n",
    "    single key 'score' and no preambel or explanation. <|eot_id|><|start_header_id|>user<|end_header_id|>\n",
    "    Here is the answer:\n",
    "    \\n -------- \\n\n",
    "    {generation}\n",
    "    \\n -------- \\n\n",
    "    Here is the question: {question}  <|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
    "\"\"\",\n",
    "input_variables=[\"generation\", \"question\"]\n",
    ")\n",
    "\n",
    "answer_grader = prompt | llm | JsonOutputParser()\n",
    "answer_grader.invoke({\"question\": question, \"generation\": generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.2 ('myenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce269a90fb56c0c7841b9bbe80dbf445b85e73251be9a478855c63ed1bb92888"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
